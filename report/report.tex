\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{COM3240 Assignment Report}

\author{\IEEEauthorblockN{Jiacong Wu}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
}

\maketitle

\section{Introduction}
In this assignment, we aim to explore how the SARSA algorithm perform, compared to Q-learning, for a reinforcement learning application to a modified game of chess on a 4x4 board, where agent aims to checkmate the opponent given an initial, random, legal board state.

\section{Task 1 - Q-Learning and SARSA}
In Q-learning, for an agent in state $s$, taking an action $a$ based on a behaviour/action selection policy, it receives reward $r$ and moves to a new state $s'$. We obtain the expected reward $Q(s',a')$ from the new state and a new action $a'$, based on the target policy (in the case of Q-learning, this target policy is always greedy \cite{sarsa yt}), which contributes to the target Q value in the update of the Q-value of a state-action pair using equation \ref{eq:q_learning_update_rule}

\begin{equation}
    Q(s,a) = Q(s,a) + \eta(r + \max_{a'}\gamma Q(s',a') - Q(s,a))
    \label{eq:q_learning_update_rule}
\end{equation}

\begin{itemize}
    \item $\gamma$ is the discount factor $0 < \gamma < 1$, adjusted depending on how much importance to put on the immediate reward and future reward. For $\gamma = 0$, the agent will only consider the immediate reward, while a factor approaching 1 will make it strive for a long-term high reward \cite{sarsa}.
    \item $\eta$ is the learning rate/step size, which is used to control the speed of learning.
\end{itemize}

The SARSA (State-action-reward-state-action) algorithm is a slightly modified version of the Q-learning algorithm, in the difference being it uses the action $a'$ selected by the behaviour policy to obtain the expected reward $Q(s',a')$ in the update of the Q-value of a state-action pair using equation \ref{eq:sarsa_q_update_rule}, instead of using a fixed greedy target policy as with Q-learning

\begin{equation}
    Q(s,a) = Q(s,a) + \eta(r + \gamma Q(s',a') - Q(s,a))
    \label{eq:sarsa_q_update_rule}
\end{equation}

And obviously, with SARSA's approach we will have the advantage of being able to use a non-greedy behaviour policy for obtaining the expected reward, which allows the algorithm to explore a range of different solutions, and not be potentially stuck in a local optimal solution as with Q-learning's greedy approach. 

\section{Task2 - Implementing SARSA}
We wrote the code to forward inference of the neural network to obtain the Q-value for each action, shown in Figure \ref{fig:forward_inference} (implementation details/code are listed in the Appendix section). The RELU activation function for the output weighted sums from the hidden layers, instead of a sigmoid function. The advantage of using RELU is that it reduces the likelihood of the vanishing gradient problem\cite{relu advantage}, arising from gradient-based such as backpropagation which is what we used to train the neural network\cite{vanishing gradient problem}. The vanishing gradient problem is when the gradient of the loss function approaches 0, causing the weights to not be updated, and therefore the neural network will fail to learn anything.

We proceed to implement the behaviour/action policy in Figure \ref{fig:behaviour_policy}. The behaviour policy is epsilon-greedy, where the agent will take a random action \texttt{a\_agent} from the array of legal actions \texttt{a}, with probability $\epsilon$, and take the greedy action with probability $1 - \epsilon$. If after taking the action, we haven't reached the terminal state, another action \texttt{a\_agent2} will be taken from the list of next legal actions, resulted taking the previous action as shown in Figure \ref{fig:target_policy_sarsa}. This is chosen according to a target policy, which is the same as the behaviour policy in the case of SARSA.

A factor of the Q-value corresponding to the next action is added to the target reward to incorporate future reward. In the case of choosing an action that reaches a terminal state (checkmate or stalemate), no future action can be taken, therefore no future is incorporated. An error signal, which is the difference between the target and actual output, is computed in order to perform backpropagation, implemented in Figure \ref{fig:backpropagation}. The weights are updated with the ADAM optimizer, which is a gradient descent optimization algorithm that helps us achieve convergence much quicker. When updating the weights for the last layer \texttt{W2}, we made sure we are only updating the weights corresponding to the action taken and not all the weights. We also implemented a function in Figure \ref{fig:exponential_moving_average} for computing the exponential moving averages for the data we collected, for the accumulated reward and number of moves taken during each episode, using a window size of 1000.

I found out that using a learning rate $\eta$ of 0.00015 instead of the default suggested value of 0.0035, gave me a much higher average reward of around 0.85, as well as having a stable convergence trend at the end of 40000 episodes, shown in Figure \ref{fig:eta_0_00015_sarsa}.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.5\textwidth]{images/eta_0_00015_sarsa.png}}
\caption{EMA of data recorded for SARSA, with $\eta=0.00015$, $\gamma=0.85$, $N\_episodes=40000$}
\label{fig:eta_0_00015_sarsa}
\end{figure}

\section{Task 3 - Varying $\gamma$ and $\epsilon$ via $\beta$}
\subsection{Varying $\gamma$}
$\gamma$ is the discount factor controlling the importance of future rewards. A higher $\gamma$ value will make the agent strive for a long-term high reward, while a lower $\gamma$ value will make the agent focus on the immediate reward. In the case of the modified chess game, a higher $\gamma$ value will make the agent focus on the long-term reward of checkmating the opponent and therefore incorporating long-term planning behaviour. While a lower $\gamma$ value will make the agent focus on immediate rewards such as threatening the opponent's king. I found using a $\gamma$ of 0.75 gave a much more smoother convergence trend (Figure \ref{fig:eta_0_00015_gamma_0_75_sarsa}) compared to other values. Increasing the value of $\gamma$, the agent was able to obtain.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.5\textwidth]{images/eta_0_00015_gamma_0_75_sarsa.png}}
\caption{EMA of data recorded for SARSA, with $\eta=0.00015$, $\gamma=0.75$, $N\_episodes=40000$}
\label{fig:eta_0_00015_gamma_0_75_sarsa}
\end{figure}

\subsection{Varying $\epsilon$ via $\beta$}
$\epsilon$ is the probability of the agent taking a random action, instead of a greedy action by choosing the action that corresponds to the highest Q-value. A lower $\epsilon$ value will make the agent more likely to take the greedy action, while a higher value will make the agent more likely to take a random action, this will allow the agent to explore different solutions rather than always exploiting what's currently best.

$\beta$ is a parameter that sets how quickly the value of $\epsilon$ is decaying. After each episode, $\epsilon$ is set using equation \ref{eq:epsilon_decay}, where $\epsilon_0$ is the initial probability and $n$ is the episode number.

\begin{equation}\label{eq:epsilon_decay}
    \epsilon_n = \frac{\epsilon_0}{1 + \beta * n}
\end{equation}

Using a lower $\beta$ value seems to allow us to reach convergence much quicker shown in Figure \ref{fig:eta_0_00015_gamma_0_75_beta_0_0000005_sarsa}, however the value converged to seems to be lower than what using a higher $\beta$ value of 0.00005 (Figure \ref{fig:eta_0_00015_gamma_0_75_beta_0_00005_sarsa}) would be, in the long run.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.5\textwidth]{images/eta_0_00015_gamma_0_75_beta_0_0000005_sarsa.png}}
\caption{EMA of data recorded for SARSA, with $\eta=0.00015$, $\gamma=0.75$, $\beta=0.0000005$, $N\_episodes=40000$}
\label{fig:eta_0_00015_gamma_0_75_beta_0_0000005_sarsa}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.5\textwidth]{images/eta_0_00015_gamma_0_75_beta_0_00005_sarsa.png}}
\caption{EMA of data recorded for SARSA, with $\eta=0.00015$, $\gamma=0.75$, $\beta=0.00005$, $N\_episodes=40000$}
\label{fig:eta_0_00015_gamma_0_75_beta_0_00005_sarsa}
\end{figure}


\section{Task 4 - Implementing Q-Learning}
Again, the Q-learning and SARSA algorithms are very similar, the only difference being the target policy used to obtain the expected reward. Q-learning's target policy is always greedy, that is, it always chooses the action that gives the highest Q-value/expected reward. We simply commented out SARSA's target policy and replaced with the line of code in Figure \ref{fig:target_policy_qlearning} to implement Q-learning.

Comparing the results obtained from Q-leraning below in Figure \ref{fig:eta_0_00015_gamma_0_75_qlearning}, with SARSA in Figure \ref{fig:eta_0_00015_gamma_0_75_sarsa}. SARSA seems to reach convergence much quicker, however that reward value seems to be lower than what Q-learning is capable of achieving in the long run, also Q-learning takes much less number of moves.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.5\textwidth]{images/eta_0_00015_gamma_0_75_qlearning.png}}
\caption{EMA of data recorded for Q-learning, with $\eta=0.00015$, $\gamma=0.75$, $\beta=0.00005$, $N\_episodes=40000$}
\label{fig:eta_0_00015_gamma_0_75_qlearning}
\end{figure}

\newpage

\begin{thebibliography}{00}
\bibitem{sarsa yt} P. Porwal, "SARSA (State Action Reward State Action) Learning - Reinforcement Learning - Machine Learning" YouTube. \url{https://www.youtube.com/watch?v=FhSaHuC0u2M&ab_channel=PankajPorwal} (Accessed: April 6, 2023)

\bibitem{sarsa} "State–action–reward–state–action", Wikipedia. \url{https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action} (Accessed: April 6, 2023)

\bibitem{relu advantage} DaemonMaker, "What are the advantages of ReLU over sigmoid function in deep neural networks?" Stack Overflow. \url{https://stats.stackexchange.com/questions/126238/what-are-the-advantages-of-relu-over-sigmoid-function-in-deep-neural-networks} (Accessed: May 16, 2023)

\bibitem{vanishing gradient problem} "The Vanishing Gradient Problem" Great Learning. \url{https://www.mygreatlearning.com/blog/the-vanishing-gradient-problem/} (Accessed: May 16, 2023)

\end{thebibliography}

\section*{Notes}
The code for this assignment can be found at \url{https://github.com/jwu02/com3240-reinforcement-learning-assignment}. We used the same seed for the Numpy random number generator to ensure reproducibility of results. Furthermore, we introduced a variable to note the number of episodes trained on if we wish to discontinue training at some point, this variable can be used with the exponential moving average function to easily verify that the results are reproducible, without changing the code. Unless explicitly stated, the parameters used to generate each of the plots can be assumed to be the default, suggested hyperparameters:
\begin{itemize}
    \item $\epsilon_0 = 0.2$
    \item $\beta = 0.000005$
    \item $\gamma = 0.85$
    \item $\eta = 0.0035$
    \item $N\_episodes = 100000$
\end{itemize}

At least 1000 episodes much be ran to produce a plot, since that's the window size we used. For switching between the Q-learning and SARSA algorithms, we simply commented out the epsilon-greedy target policy for SARSA and replaced it with the greedy target policy for Q-learning, and vice versa.

\newpage
\appendix

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.45\textwidth]{images/forward_inference.png}}
\caption{Forward inference/neural activation of neural network to calculate Q-value for each action}
\label{fig:forward_inference}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.45\textwidth]{images/behaviour_policy.png}}
\caption{Implementing epsilon-greedy behaviour policy}
\label{fig:behaviour_policy}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.45\textwidth]{images/target_policy_sarsa.png}}
\caption{Implementing epsilon-greedy target policy for SARSA}
\label{fig:target_policy_sarsa}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.45\textwidth]{images/backpropagation.png}}
\caption{Backpropagation and update weights with ADAM optimizer}
\label{fig:backpropagation}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.45\textwidth]{images/ema.png}}
\caption{Function for computing exponential moving averages}
\label{fig:exponential_moving_average}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.45\textwidth]{images/target_policy_qlearning.png}}
\caption{Implementing greedy target policy for Q-learning}
\label{fig:target_policy_qlearning}
\end{figure}

\end{document}
